{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from edward.models import Normal\n",
    "from edward.models import Bernoulli\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "\"\"\"\n",
    "Helper methods\n",
    "\"\"\"\n",
    "##creates a weight variable with name and shape\n",
    "def _weight_variable(name, shape):\n",
    "    w = tf.get_variable(name, shape, tf.float32, tf.truncated_normal_initializer(stddev=0.1))\n",
    "    tf.add_to_collection('vars',w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def _bias_variable(name, shape):\n",
    "    b = tf.get_variable(name, shape,tf.float32, tf.constant_initializer(0.1, dtype=tf.float32))\n",
    "    tf.add_to_collection('vars',b)\n",
    "    return b\n",
    "\n",
    "def _get3d_deconv_output_size(input_height, input_width,input_depth, filter_height,\n",
    "                             filter_width,filter_depth, row_stride, col_stride,depth_stride,\n",
    "                             padding_type):\n",
    "    \"\"\"Returns the number of rows and columns in a convolution/pooling output.\n",
    "    \"\"\"\n",
    "    input_height = tensor_shape.as_dimension(input_height)\n",
    "    input_width = tensor_shape.as_dimension(input_width)\n",
    "    input_depth = tensor_shape.as_dimension(input_depth)\n",
    "    filter_height = tensor_shape.as_dimension(filter_height)\n",
    "    filter_width = tensor_shape.as_dimension(filter_width)\n",
    "    filter_depth = tensor_shape.as_dimension(filter_depth)\n",
    "    row_stride = int(row_stride)\n",
    "    col_stride = int(col_stride)\n",
    "    depth_stride = int(depth_stride)\n",
    "\n",
    "    # Compute number of rows in the output, based on the padding.\n",
    "    if input_height.value is None or filter_height.value is None:\n",
    "        out_rows = None\n",
    "    elif padding_type == \"VALID\":\n",
    "        out_rows = (input_height.value - 1) * row_stride + filter_height.value\n",
    "    elif padding_type == \"SAME\":\n",
    "        out_rows = input_height.value * row_stride\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for padding: %r\" % padding_type)\n",
    "\n",
    "    # Compute number of columns in the output, based on the padding.\n",
    "    if input_width.value is None or filter_width.value is None:\n",
    "        out_cols = None\n",
    "    elif padding_type == \"VALID\":\n",
    "        out_cols = (input_width.value - 1) * col_stride + filter_width.value\n",
    "    elif padding_type == \"SAME\":\n",
    "        out_cols = input_width.value * col_stride\n",
    "        \n",
    "    # Compute number of columns in the output, based on the padding.\n",
    "    if input_depth.value is None or filter_depth.value is None:\n",
    "        out_depth = None\n",
    "    elif padding_type == \"VALID\":\n",
    "        out_depth = (input_depth.value - 1) * depth_stride + filter_depth.value\n",
    "    elif padding_type == \"SAME\":\n",
    "        out_depth = input_depth.value * depth_stride\n",
    "\n",
    "\n",
    "    return out_rows, out_cols,out_depth\n",
    "\n",
    "\n",
    "## creates a deconvolution layer with specific activation function\n",
    "def _3D_deconv_layer(name, kernel_shape, biases_shape,prev_layer, activation_fn = tf.nn.softplus,\n",
    "                  strides = [1, 1, 1,1,1], padding = 'VALID'):\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = _weight_variable('weights',kernel_shape)\n",
    "        biases = _bias_variable('biases',biases_shape)\n",
    "        \n",
    "        out_row,out_col,out_depth = _get3d_deconv_output_size(prev_layer.shape[1],prev_layer.shape[2],prev_layer.shape[3],kernel_shape[0],kernel_shape[1],kernel_shape[2],strides[1],strides[2],strides[3],padding)\n",
    "        output_shape = [batch_size,out_row,out_col,out_depth,kernel_shape[3]]\n",
    "        conv = tf.nn.conv3d_transpose(prev_layer,kernel,output_shape, strides,padding=padding,name=scope.name)\n",
    "        conv = tf.nn.bias_add(conv,biases)\n",
    "        if activation_fn is not None:\n",
    "            conv = activation_fn(conv)\n",
    "    return conv\n",
    "\n",
    "##creates a convolution layer with specific activation function\n",
    "def _3D_conv_layer(name, kernel_shape, biases_shape,prev_layer, activation_fn = tf.nn.softplus,\n",
    "                  strides = [1, 1, 1, 1, 1], padding = 'VALID'):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = _weight_variable('weights',kernel_shape)\n",
    "        biases = _bias_variable('biases',biases_shape)\n",
    "        conv = tf.nn.conv3d(prev_layer,kernel,strides,padding=padding)\n",
    "        conv = tf.nn.bias_add(conv,biases)\n",
    "        if activation_fn is not None:\n",
    "            conv = activation_fn(conv,name = scope.name)\n",
    "    return conv\n",
    "\n",
    "\n",
    "# we take a random variable z~N(0,1)\n",
    "# pass through deconvolutional network\n",
    "# to produce a mean and sigma for each voxel\n",
    "def generative_network(z):\n",
    "    \"\"\"Generative network to parameterize the generative model\n",
    "    It takes latent variables as input and outputs the likelihood parameters\n",
    "        \n",
    "    mu = neural_network(z) sigma = neural_network(z)\n",
    "    \"\"\"\n",
    "    reshape_z = tf.reshape(z,[batch_size,1,1,1,latent_dimension])\n",
    "    prev_layer = reshape_z\n",
    "    in_filter = latent_dimension\n",
    "    #deconv layer 1\n",
    "    out_filter=1\n",
    "    kernel_shape = [31,37,31,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv1',kernel_shape,biases_shape,prev_layer,activation_fn=tf.nn.sigmoid)\n",
    "    in_filter = out_filter\n",
    "    \"\"\"\n",
    "    #deconv layer 2\n",
    "    out_filter=10\n",
    "    kernel_shape = [4,5,4,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv2',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "\n",
    "    #deconv layer 3\n",
    "    out_filter=5\n",
    "    kernel_shape = [5,5,5,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv3',kernel_shape,biases_shape,prev_layer,strides = [1,2,2,2,1])\n",
    "    in_filter = out_filter\n",
    "    #deconv layer 4\n",
    "    out_filter=1\n",
    "    kernel_shape = [3,3,3,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv4',kernel_shape,biases_shape,prev_layer,strides = [1,2,2,2,1],padding = 'SAME')\n",
    "    in_filter = out_filter\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #deconv layer 5\n",
    "    out_filter=3\n",
    "    kernel_shape = [5,7,5,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv5',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "   \n",
    "    #deconv layer 6\n",
    "    out_filter = 12\n",
    "    kernel_shape  =[10,10,10,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_deconv_layer('deconv6',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #output_layer\n",
    "    mu = tf.reshape(fully_connected[:, :latent_dimension],[batch_size,latent_dimension])\n",
    "    out_filter = 1\n",
    "    kernel_shape = [2,4,2,out_filter,in_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    output = _3D_deconv_layer('gen_output',kernel_shape,biases_shape,prev_layer,activation_fn=tf.nn.sigmoid)\n",
    "    \"\"\"\n",
    "    mu = tf.reshape(prev_layer,[batch_size,x_shape[0],x_shape[1],x_shape[2],1])\n",
    "    #mu = tf.reshape(fully_connected,[batch_size,x_shape[0],x_shape[1],x_shape[2],1])\n",
    "    #sigma =  tf.reshape(tf.nn.softplus(output[:,:,:,:,1]),[batch_size,x_shape[0],x_shape[1],x_shape[2],1]) + 1e-10\n",
    "\n",
    "    return mu#,sigma\n",
    "\n",
    "def inference_network(x,latent_dimension = 100):\n",
    "    \"\"\"Inference network to parameterize the variational family\n",
    "    it takes data as input and outputs the variational parameters\n",
    "    \n",
    "    \n",
    "    the network follows the architecture given by \n",
    "    Deep MRI brain extraction: A 3D convolutional neural network for\n",
    "skull stripping Kleesiek, J 2016\n",
    "\n",
    "    mu,sigma = encoder_neural_network(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_dimension = latent_dimension\n",
    "    in_filter = 1\n",
    "    #reshaped_x = tf.reshape(x,[batch_size,tf.shape(x)[1],tf.shape(x)[2],tf.shape(x)[3],in_filter])\n",
    "    #layer 1\n",
    "    out_filter = 24\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv1',kernel_shape,biases_shape,x)\n",
    "    in_filter = out_filter\n",
    "    \n",
    "    #do max pooling\n",
    "    pool1 = tf.nn.max_pool3d(prev_layer,ksize = [1,2,2,2,1],\n",
    "                             strides = [1,2,2,2,1],\n",
    "                            padding= 'SAME')\n",
    "    \n",
    "    prev_layer = pool1\n",
    "    \n",
    "    #layer 2\n",
    "    out_filter = 28\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv2',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "    \n",
    "    \n",
    "    #layer 3\n",
    "    out_filter = 24\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv3',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "    \n",
    "    #layer 4\n",
    "    out_filter = 34\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv4',kernel_shape,biases_shape,prev_layer,strides = [1,2,2,2,1])\n",
    "    in_filter = out_filter\n",
    "\n",
    "    #layer 5\n",
    "    out_filter = 42\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv5',kernel_shape,biases_shape,prev_layer,activation_fn=None)\n",
    "    in_filter = out_filter\n",
    "    \"\"\"\n",
    "    #layer 6\n",
    "    out_filter = 50\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv6',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "    #layer 7\n",
    "    out_filter = 50\n",
    "    kernel_shape = [3,3,3,in_filter,out_filter]\n",
    "    biases_shape = [out_filter]\n",
    "    prev_layer = _3D_conv_layer('conv7',kernel_shape,biases_shape,prev_layer)\n",
    "    in_filter = out_filter\n",
    "    \"\"\"\n",
    "    #fully connected layer\n",
    "    dim = tensor_shape.as_dimension(prev_layer.shape[1])*tensor_shape.as_dimension(prev_layer.shape[2])*tensor_shape.as_dimension(prev_layer.shape[3])*tensor_shape.as_dimension(prev_layer.shape[4])\n",
    "    dim = int(dim)\n",
    "    prev_layer_flat = tf.reshape(prev_layer, [batch_size, dim])\n",
    "\n",
    "    weights = _weight_variable('weights_full', [dim, latent_dimension*2])\n",
    "    biases = _bias_variable('biases_full', [latent_dimension*2])\n",
    "    fully_connected = tf.matmul(prev_layer_flat, weights) + biases\n",
    "    \n",
    "    mu = tf.reshape(fully_connected[:, :latent_dimension],[batch_size,latent_dimension])\n",
    "    logsigma2 = tf.reshape(fully_connected[:,latent_dimension:],[batch_size,latent_dimension])\n",
    "    \"\"\"\n",
    "    fully_connected = prev_layer\n",
    "    mu = tf.reshape(fully_connected[:,:,:,:,0],[batch_size,latent_dimesnion])\n",
    "    signa = tf.reshape(tf.nn.softplus(fully_connected[:,:,:,:,1]),[batch_size,latent_dimension])+1e-10\n",
    "    \"\"\"\n",
    "    return mu,logsigma2\n",
    "\n",
    "def VAE_loss(x_ph,x_mu,z_mu,z_logsigma2,learning_rate = 0.01):\n",
    "    reconstruct_loss = -tf.reduce_sum(x_ph*tf.log(x_mu +1e-8) + \\\n",
    "                                    (1-x_ph)*tf.log(1-x_mu+1e-8),axis = 1)\n",
    "    \n",
    "    latent_loss = -0.5*tf.reduce_sum(1+z_logsigma2\n",
    "                               - tf.square(z_mu)\n",
    "                               -tf.exp(z_logsigma2),axis=1)\n",
    "    cost = tf.reduce_mean(latent_loss,axis=0)\n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    return cost,optimiser\n",
    "\n",
    "def read_and_decode_single_example(filename):\n",
    "    # first construct a queue containing a list of filenames.\n",
    "    # this lets a user split up there dataset in multiple files to keep\n",
    "    # size down\n",
    "    filename_queue = tf.train.string_input_producer([filename],\n",
    "                                                    num_epochs=None)\n",
    "    # Unlike the TFRecordWriter, the TFRecordReader is symbolic\n",
    "    reader = tf.TFRecordReader()\n",
    "    # One can read a single serialized example from a filename\n",
    "    # serialized_example is a Tensor of type string.\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # The serialized example is converted back to actual values.\n",
    "    # One needs to describe the format of the objects to be returned\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            # We know the length of both fields. If not the\n",
    "            # tf.VarLenFeature could be used\n",
    "            'image': tf.FixedLenFeature([x_shape[0]*x_shape[1]*x_shape[2]], tf.float32)\n",
    "        })\n",
    "    # now return the converted data\n",
    "    image = features['image']\n",
    "    return image\n",
    "\"\"\"\n",
    " Global variables\n",
    "\"\"\"\n",
    "ed.set_seed(42)\n",
    "latent_dimension = 100\n",
    "batch_size = 4\n",
    "#x_shape = [91,109,91]\n",
    "x_shape = [31,37,31]\n",
    "\n",
    "\"\"\"\n",
    "The model\n",
    "\"\"\"\n",
    "\n",
    "x_ph = tf.placeholder(tf.float32,[None, x_shape[0],x_shape[1],x_shape[2],1])\n",
    "z_mu,z_logsigma2 = inference_network(x_ph)\n",
    "epsilon = tf.random_normal([batch_size,latent_dimension],0,1,dtype=tf.float32)\n",
    "z = tf.add(tf.multiply(tf.sqrt(tf.exp(z_logsigma2)),epsilon),z_mu)\n",
    "x_mu = generative_network(z)\n",
    "cost,optimiser = VAE_loss(x_ph,x_mu,z_mu,z_logsigma2)\n",
    "\n",
    "## IMPORt the data\n",
    "# returns symbolic label and image\n",
    "image = read_and_decode_single_example(\"T1_mri_full_BL_normalized.tfrecords\")\n",
    "# groups examples into batches randomly\n",
    "images_batch = tf.train.shuffle_batch(\n",
    "    [image], batch_size=batch_size,\n",
    "    capacity=20,\n",
    "    min_after_dequeue=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  ..., \n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  ..., \n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  ..., \n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  ..., \n",
      "  [[[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 0.]]]\n",
      "\n",
      "\n",
      "  [[[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 0.]\n",
      "    [ 0.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    [ 0.]]\n",
      "\n",
      "   ..., \n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    [ 1.]]\n",
      "\n",
      "   [[ 1.]\n",
      "    [ 0.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]\n",
      "\n",
      "   [[ 0.]\n",
      "    [ 1.]\n",
      "    [ 1.]\n",
      "    ..., \n",
      "    [ 1.]\n",
      "    [ 0.]\n",
      "    [ 0.]]]]]\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n",
      "\n",
      "Caused by op u'input_producer/input_producer_EnqueueMany', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/hngu4068/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 498, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-299703fb7f8f>\", line 312, in <module>\n",
      "    image = read_and_decode_single_example(\"T1_mri_full_BL_normalized.tfrecords\")\n",
      "  File \"<ipython-input-1-299703fb7f8f>\", line 272, in read_and_decode_single_example\n",
      "    num_epochs=None)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n",
      "    cancel_op=cancel_op)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 162, in input_producer\n",
      "    enq = q.enqueue_many([input_tensor])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 378, in enqueue_many\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1538, in _queue_enqueue_many_v2\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Lets train the model!\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=session,coord=coord)\n",
    "    n_epoch = 1\n",
    "    n_iter_per_epoch = 1\n",
    "    for epoch in range(n_epoch):\n",
    "        avg_loss = 0.0\n",
    "        for t in range(n_iter_per_epoch):\n",
    "            x_train= session.run([images_batch])[0]\n",
    "            x_train = np.reshape(x_train,(batch_size,x_shape[0],x_shape[1],x_shape[2],1))\n",
    "            temp = session.run(x_mu,feed_dict={x_ph:x_train})\n",
    "            print(temp)\n",
    "            #print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session =tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, '/home/hngu4068/Documents/mri_vae/mri_vae_model/my-model.ckpt-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train= session.run([images_batch])[0]\n",
    "x_train = np.reshape(x_train,(batch_size,x_shape[0],x_shape[1],x_shape[2],1))\n",
    "reconstruct_x = session.run(mu_x, feed_dict = {x_ph:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruct_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.rot90(reconstruct_x[0,:,:,10,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruct_x[0,:,:,15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.rot90(x_train[0,:,:,10,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
